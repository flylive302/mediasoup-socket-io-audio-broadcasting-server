# ELITE DOMAIN FORENSIC AUDIT — Seat Domain

## 1. Executive Summary

The seat domain is **architecturally solid** — Lua scripts for atomicity, Redis-backed state for horizontal scaling, `createHandler` for consistent validation/logging, and clean file-per-handler separation. However, a forensic deep-dive reveals **5 CRITICAL/HIGH findings** that represent real production risk (race conditions, memory leaks, redundant Redis round-trips that could be eliminated) and **9 MEDIUM/LOW findings** spanning dead code, missing index lookups, inconsistent auth policy, and Zod schema rigidity.

**Bottom line:** The domain works correctly in the happy path but has several latent issues that will surface under load, at scale, or during edge-case user behavior.

## 2. Context Metadata

| Key          | Value                                                                      |
| ------------ | -------------------------------------------------------------------------- |
| Commit       | `a4db816`                                                                  |
| Branch       | `work`                                                                     |
| Node Version | `v24.12.0`                                                                 |
| Audit Scope  | `src/domains/seat/` (15 files)                                             |
| Test File    | `tests/unit/domains/seat/seat.repository.test.ts`                          |
| Cross-Refs   | `room.handler.ts`, `socket/index.ts`, `roomManager.ts`, `clientManager.ts` |

## 3. Domain Coverage Matrix

| File                                  | Lines | Audit Status       |
| ------------------------------------- | ----- | ------------------ |
| `seat.handler.ts`                     | 51    | ✅ Reviewed        |
| `seat.repository.ts`                  | 639   | ✅ Reviewed (deep) |
| `seat.types.ts`                       | 27    | ✅ Reviewed        |
| `seat.owner.ts`                       | 157   | ✅ Reviewed        |
| `index.ts`                            | 23    | ✅ Reviewed        |
| `handlers/take-seat.handler.ts`       | 43    | ✅ Reviewed        |
| `handlers/leave-seat.handler.ts`      | 35    | ✅ Reviewed        |
| `handlers/assign-seat.handler.ts`     | 51    | ✅ Reviewed        |
| `handlers/remove-seat.handler.ts`     | 39    | ✅ Reviewed        |
| `handlers/mute-seat.handler.ts`       | 78    | ✅ Reviewed        |
| `handlers/unmute-seat.handler.ts`     | 78    | ✅ Reviewed        |
| `handlers/lock-seat.handler.ts`       | 78    | ✅ Reviewed        |
| `handlers/unlock-seat.handler.ts`     | 43    | ✅ Reviewed        |
| `handlers/invite-seat.handler.ts`     | 106   | ✅ Reviewed        |
| `handlers/invite-response.handler.ts` | 147   | ✅ Reviewed        |
| `seat.repository.test.ts`             | 404   | ✅ Reviewed        |

---

## 4. Findings by Dimension (sorted by severity)

---

### FINDING SEAT-001 — CRITICAL: `lock-seat.handler.ts` isSeatLocked + lockSeat = TOCTOU Race

```
Severity: CRITICAL
File: src/domains/seat/handlers/lock-seat.handler.ts
Function: lockSeatHandler
Problem: Pre-check isSeatLocked() + lockSeat() is a classic TOCTOU (time-of-check/time-of-use) race
Evidence: `L24: isSeatLocked(roomId, seatIndex)` → `L33: lockSeat(roomId, seatIndex)`
Impact: Two concurrent lock requests can both pass the check; second SADD is idempotent but the redundant "already locked" guard is defeated — wasted auth API call + extra Redis round-trip
Fix: Move "already locked" check INTO the Lua lockSeat script (SISMEMBER + SADD atomic) and return a distinct code like ALREADY_LOCKED. Eliminates the separate isSeatLocked() call entirely
Complexity: 1h
```

---

### FINDING SEAT-002 — CRITICAL: `unlock-seat.handler.ts` isSeatLocked + unlockSeat = Same TOCTOU

```
Severity: CRITICAL
File: src/domains/seat/handlers/unlock-seat.handler.ts
Function: unlockSeatHandler
Problem: Same TOCTOU pattern — pre-check isSeatLocked() then unlockSeat() are two separate Redis ops
Evidence: `L24: isSeatLocked()` → `L33: unlockSeat()`
Impact: Concurrent unlock requests can both pass check; the SREM is idempotent so no data corruption, but wasted auth call + misleading response
Fix: Create Lua script for unlock that atomically checks + removes from locked set, returning ALREADY_UNLOCKED if not found
Complexity: 1h
```

---

### FINDING SEAT-003 — HIGH: `invite-response.handler.ts` accept flow has 4 sequential non-atomic Redis ops

```
Severity: HIGH
File: src/domains/seat/handlers/invite-response.handler.ts
Function: inviteAcceptHandler
Problem: deleteInvite → isSeatLocked → unlockSeat → takeSeat = 4 sequential Redis round-trips with race windows between each
Evidence: L52 deleteInvite, L61 isSeatLocked, L63 unlockSeat, L69 takeSeat
Impact: Between deleting the invite and taking the seat, another user could take the seat via seat:take. The invite user gets a confusing "seat taken" error after accepting a valid invite
Fix: Create an atomic Lua script for invite-accept: verify invite, delete it, unlock if locked, take seat — all in one EVALSHA
Complexity: 2h
```

---

### FINDING SEAT-004 — HIGH: `roomOwnerCache` in `seat.owner.ts` is an unbounded in-memory Map (memory leak)

```
Severity: HIGH
File: src/domains/seat/seat.owner.ts
Function: roomOwnerCache (module-level Map)
Problem: Map grows unboundedly — entries are only deleted via clearRoomOwner() which is exported but NEVER CALLED anywhere in the codebase
Evidence: grep for `clearRoomOwner` returns only its definition and export — zero call sites
Impact: Over time, every room that's ever created adds an entry. With TTL expiry the data goes stale but the Map entry (key + object) is never GC'd. On a long-running server with high room churn → memory leak
Fix: (a) Call clearRoomOwner() in roomManager.closeRoom() when cleaning up. (b) Add periodic pruning of expired entries (e.g. setInterval every 60s to evict expiresAt < now). (c) Or use a bounded LRU cache
Complexity: 1h
```

---

### FINDING SEAT-005 — HIGH: `getUserSeat()` scans ALL seats via HGETALL — O(n) with no reverse index

```
Severity: HIGH
File: src/domains/seat/seat.repository.ts
Function: getUserSeat
Problem: Fetches HGETALL (all seats for a room), then iterates in JS to find user — O(seatCount) with JSON.parse on every entry
Evidence: `L435: hgetall(SEATS_KEY(roomId))` + `L437-441: for loop with JSON.parse`
Impact: Called on every mute/unmute action. With 15 seats this is small, but architecturally it's the wrong pattern — invite system already has a reverse index pattern. If seat count ever grows, so does latency
Fix: Add a reverse index: `room:{roomId}:seat:user:{userId}` → seatIndex (like the invite reverse index). Write it atomically in the Lua take/assign/leave scripts. getUserSeat becomes a single HGET O(1) lookup
Complexity: 2h
```

---

### FINDING SEAT-006 — MEDIUM: `clearRoomOwner()` is dead code — exported but never imported/called

```
Severity: MEDIUM
File: src/domains/seat/seat.owner.ts + index.ts
Function: clearRoomOwner
Problem: Exported from both seat.owner.ts and index.ts barrel, but zero call sites exist in the entire codebase
Evidence: grep returns only definition (L38) and barrel export (L19) — no imports
Impact: Dead code. Also the root cause of SEAT-004 (unbounded cache), since room closure never cleans up the cache entry
Fix: Wire clearRoomOwner() into roomManager.closeRoom() cleanup path. If intentionally unused, remove it
Complexity: 0.5h
```

---

### FINDING SEAT-007 — MEDIUM: `SEAT_UNAVAILABLE` error is defined but never used

```
Severity: MEDIUM
File: src/shared/errors.ts
Function: Errors.SEAT_UNAVAILABLE
Problem: Defined at L23 but zero references in any handler or repository in the entire src/ tree
Evidence: grep for SEAT_UNAVAILABLE returns only the definition in errors.ts
Impact: Dead code — adds cognitive burden when reading error definitions
Fix: Remove from errors.ts, or use it in invite-accept handler when the seat gets taken between invite and acceptance
Complexity: 0.25h
```

---

### FINDING SEAT-008 — MEDIUM: Hardcoded `.max(14)` in Zod schemas contradicts dynamic `seatCount`

```
Severity: MEDIUM
File: src/socket/schemas.ts
Function: seatTakeSchema, seatAssignSchema, seatLockSchema, seatInviteSchema
Problem: Schemas hardcode `.max(14)` for seatIndex, but rooms support dynamic seatCount (1–15 via joinRoomSchema). If seatCount is ever raised above 15, schema validation will reject valid seat indices
Evidence: `seatIndex: z.number().int().min(0).max(14)` across 5 schemas
Impact: Tight coupling — changing max seat count requires updating both config AND every schema
Fix: Either: (a) Remove .max(14) from schemas and let Lua script validate bounds with seatCount, or (b) Make max a const imported from config and reference it in all schemas
Complexity: 0.5h
```

---

### FINDING SEAT-009 — MEDIUM: `config.DEFAULT_SEAT_COUNT` used instead of actual room seatCount in take/assign/invite-accept

```
Severity: MEDIUM
File: handlers/take-seat.handler.ts, assign-seat.handler.ts, invite-response.handler.ts
Function: takeSeatHandler, assignSeatHandler, inviteAcceptHandler
Problem: Passes `config.DEFAULT_SEAT_COUNT` (always 15) to Lua scripts, but rooms can have custom seatCount persisted in room state. A room with seatCount=8 would still accept seat:take at index 12
Evidence: `config.DEFAULT_SEAT_COUNT` on L21, L29, L73 respectively
Impact: Business logic bypass — users can sit in seats that don't exist for that room's configuration
Fix: Read actual seatCount from `roomManager.state.get(roomId)` and pass it to the Lua scripts. Cache it on the room state to avoid extra Redis reads
Complexity: 1h
```

---

### FINDING SEAT-010 — MEDIUM: `mute-seat.handler.ts` and `unmute-seat.handler.ts` are 95% identical — code duplication

```
Severity: MEDIUM
File: handlers/mute-seat.handler.ts + unmute-seat.handler.ts
Function: muteSeatHandler + unmuteSeatHandler
Problem: Two handlers with near-identical structure. The only differences are: muted=true/false, producer.pause/resume, event isMuted value, and error message
Evidence: Both are 78 lines, same validation, same auth, same getUserSeat, same producer lookup pattern
Impact: Maintenance burden — any fix to the mute flow (e.g. getUserSeat → reverse index) must be applied to both files
Fix: Extract a shared `setMuteHandler(muted: boolean)` factory that handles auth, seat lookup, Redis update, producer control, and emit. Both handlers become one-liners
Complexity: 1h
```

---

### FINDING SEAT-011 — MEDIUM: `invite-seat.handler.ts` doesn't check if target user is already seated

```
Severity: MEDIUM
File: handlers/invite-seat.handler.ts
Function: inviteSeatHandler
Problem: Checks if seat is occupied and if there's a pending invite, but does NOT check if the target user is already seated elsewhere. An owner can invite a user who is already on another seat — accepting would silently move them
Evidence: No call to `getUserSeat(roomId, targetUserIdStr)` before creating invite
Impact: Confusing UX — user accepts invite and their old seat becomes vacant without an explicit "seat:cleared" for the old index
Fix: Add getUserSeat check before creating invite. If target is already seated, either: (a) reject with "User already seated", or (b) warn in the invite payload so UI can show "Will move from seat X"
Complexity: 0.5h
```

---

### FINDING SEAT-012 — LOW: Inconsistent auth policy — assign/remove use `verifyRoomOwner` while mute/unmute/lock/unlock/invite use `verifyRoomManager`

```
Severity: LOW
File: Multiple handlers
Function: assignSeatHandler, removeSeatHandler vs muteSeatHandler, lockSeatHandler, etc.
Problem: Some owner actions require strict owner check (assign, remove) while others allow admins (mute, lock, invite). This may be intentional business logic, but it's undocumented and inconsistent
Evidence: assign/remove import verifyRoomOwner; mute/unmute/lock/unlock/invite import verifyRoomManager
Impact: If assign/remove are meant to be owner-only, this is correct but should be documented. If admins should also assign/remove, it's a bug
Fix: Document the auth policy. Consider using verifyRoomManager for all owner actions for consistency, OR add a JSDoc comment explaining why assign/remove are owner-only
Complexity: 0.25h
```

---

### FINDING SEAT-013 — LOW: `seat.handler.ts` logs "Seat handlers registered" on every socket connection

```
Severity: LOW
File: src/domains/seat/seat.handler.ts
Function: registerSeatHandlers
Problem: Logs at INFO level for every socket connection. In a room with 100 users, this creates 100 log lines just for handler registration
Evidence: `L32: logger.info({ socketId, userId }, "Seat handlers registered")`
Impact: Log noise in production — handler registration is not an actionable event
Fix: Change to logger.debug or remove entirely
Complexity: 0.1h
```

---

### FINDING SEAT-014 — LOW: `deleteInvite()` reads invite data before deleting — extra GET for reverse index cleanup

```
Severity: LOW
File: src/domains/seat/seat.repository.ts
Function: deleteInvite
Problem: Does a GET to read the invite (to extract targetUserId for reverse index cleanup), then DEL in a pipeline. This is 2 round-trips. Could be done atomically in a Lua script
Evidence: `L520: redis.get(INVITE_KEY)` → `L521-529: pipeline.del()`
Impact: Extra Redis round-trip per invite delete. Minor latency but adds up under load
Fix: Create a Lua script: GET invite → DEL invite key → DEL reverse index key, all atomic in one EVALSHA
Complexity: 0.5h
```

---

## 5. Over-Engineering Penalties

```
File: src/domains/seat/seat.owner.ts
Anti-pattern: withAuthCheck() wrapper adds timing + logging overhead to every auth call
Runtime cost: ~0.5ms per call (Date.now + logger.info × 2 per check)
Dev-hours/year: 2
Score deduction: 1
```

> **Note:** This is borderline — the centralized logging is useful for debugging auth issues. The cost is small. Not recommending removal but flagging for awareness.

---

## 6. Aggregate Scores

| Dimension                  | Weight | Score (0–10) | Weighted |
| -------------------------- | ------ | ------------ | -------- |
| Performance & Efficiency   | 30     | 6.5          | 19.5     |
| Architecture & Scalability | 20     | 7.5          | 15.0     |
| Realtime Correctness       | 15     | 6.0          | 9.0      |
| Code Quality               | 15     | 7.0          | 10.5     |
| Security & Reliability     | 10     | 7.5          | 7.5      |
| Readability                | 10     | 8.0          | 8.0      |
| **Total**                  | 100    |              | **69.5** |

**Over-Engineering Deduction:** −1 → **Final: 68.5 / 100**

---

## 7. Priority Remediation Queue

| Priority | ID       | Severity | Complexity | Description                                                        |
| -------- | -------- | -------- | ---------- | ------------------------------------------------------------------ |
| P0       | SEAT-004 | HIGH     | 1h         | Wire `clearRoomOwner()` into room close + add cache pruning        |
| P0       | SEAT-009 | MEDIUM   | 1h         | Use actual room `seatCount` instead of `config.DEFAULT_SEAT_COUNT` |
| P1       | SEAT-001 | CRITICAL | 1h         | Merge `isSeatLocked` check into `lockSeat` Lua script              |
| P1       | SEAT-002 | CRITICAL | 1h         | Create atomic `unlockSeat` Lua script                              |
| P1       | SEAT-003 | HIGH     | 2h         | Atomic Lua script for invite-accept flow                           |
| P2       | SEAT-005 | HIGH     | 2h         | Add user→seat reverse index for O(1) `getUserSeat`                 |
| P2       | SEAT-010 | MEDIUM   | 1h         | Extract shared mute/unmute handler factory                         |
| P3       | SEAT-006 | MEDIUM   | 0.5h       | Remove or wire `clearRoomOwner`                                    |
| P3       | SEAT-007 | MEDIUM   | 0.25h      | Remove dead `SEAT_UNAVAILABLE` error                               |
| P3       | SEAT-008 | MEDIUM   | 0.5h       | Fix hardcoded `.max(14)` in schemas                                |
| P3       | SEAT-011 | MEDIUM   | 0.5h       | Check if target user already seated before invite                  |
| P4       | SEAT-012 | LOW      | 0.25h      | Document auth policy differences                                   |
| P4       | SEAT-013 | LOW      | 0.1h       | Downgrade handler registration log to debug                        |
| P4       | SEAT-014 | LOW      | 0.5h       | Lua script for atomic invite delete                                |

**Total estimated remediation: ~11.6 hours**

---

## 8. Performance Optimization — "Could We Squeeze More?"

### 8a. User→Seat Reverse Index (SEAT-005)

Current `getUserSeat()` does `HGETALL` + JS loop on every mute/unmute. With a reverse index `room:{roomId}:seat:user:{userId}` → seatIndex, this becomes a single `HGET` — **O(1) instead of O(n)**. The invite system already uses this exact pattern (`INVITE_USER_KEY`), proving it works. The reverse index would be maintained atomically in the take/assign/leave Lua scripts.

### 8b. Eliminate Redundant Pre-checks (SEAT-001, SEAT-002)

The `isSeatLocked()` pre-check before `lockSeat()` / `unlockSeat()` burns an extra Redis round-trip. Moving the check INTO the Lua script saves **1 RTT per lock/unlock call** — that's ~0.5ms per operation on a local Redis, more over network.

### 8c. Batch the Invite-Accept Flow (SEAT-003)

The accept handler makes 4 sequential Redis calls. A single Lua script reduces this to **1 RTT** — saving ~1.5ms under normal conditions, more under Redis latency. Critically, it also eliminates the race window.

### 8d. Per-Room Seat Count (SEAT-009)

Currently `config.DEFAULT_SEAT_COUNT` (15) is passed everywhere, meaning the Lua scripts always validate against 15 seats even for rooms with fewer. This isn't just a correctness issue — it also means the `getSeats()` method iterates over phantom seat indices, doing 15 object constructions even for an 8-seat room.

### 8e. Shared Mute/Unmute Factory (SEAT-010)

Not a runtime performance win, but eliminates the risk of drift between two near-identical files. Any future optimization (e.g. reverse index for getUserSeat) only needs to be applied once.

### 8f. `getSeats()` + `getLockedSeats()` Redundancy in `room.handler.ts`

In `room.handler.ts:L115-117`, both `getSeats()` and `getLockedSeats()` are called in parallel. But `getSeats()` already fetches locked seats internally via `smembers(LOCKED_KEY)` and maps them into each `SeatData.locked` field. The separate `getLockedSeats()` call is redundant — the locked data is already embedded in the returned `SeatData[]`. This is **1 wasted Redis SMEMBERS call per room join**.
